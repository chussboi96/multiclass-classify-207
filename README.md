This repository contains a comparative analysis of various BERT-based models for multiclass classification across 207 classes.

## Contents
1. DistilBERT Training Notebook: Code implementation for training a DistilBERT model. Training isn't fully completed due to resource limitations, but the entire training pipeline is included.
2. XLNet Training Notebook: Code implementation for training an XLNet model. Similar to the DistilBERT notebook, the training process is not fully complete due to resource constraints.


## Future Work
Planned Experiments: Future experiments will include training with MobileBERT, TinyBERT, and RoBERTa models.
