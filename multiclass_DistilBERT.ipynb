{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gNCIHouzWku"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the data\n",
        "file_path = \"C:\\\\Users\\\\dilaw\\\\OneDrive\\\\Desktop\\\\multiclass SUBCAT.csv\"\n",
        "df = pd.read_csv(file_path, header=None, names=['class', 'description'])"
      ],
      "metadata": {
        "id": "XKu_h1So2ADi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Remove NaN values and duplicates\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "C_07ANK82Clo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Data Cleaning\n",
        "df['description'] = df['description'].str.lower()\n"
      ],
      "metadata": {
        "id": "-J1bhlae2IeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['word_count'] = data['description'].str.split().str.len()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(data['word_count'], bins=range(1, data['word_count'].max()+1), alpha=0.7, color='blue')\n",
        "plt.title('Distribution of Number of Words')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(1, data['word_count'].max()+1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6fYfq3Y82PLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Encode the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['encoded_class'] = label_encoder.fit_transform(df['class'])"
      ],
      "metadata": {
        "id": "yPAZmN1D2Qjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GoM5N6l12R_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare the data\n",
        "data_texts = df['description'].tolist()\n",
        "data_labels = df['encoded_class'].tolist()"
      ],
      "metadata": {
        "id": "zZdXnxvH2TKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step7: Splitting data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0)"
      ],
      "metadata": {
        "id": "hDTL6UMP4F-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step8: Tokenization\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n"
      ],
      "metadata": {
        "id": "ME6rYbOI4F7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step9: Convert to TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\n"
      ],
      "metadata": {
        "id": "NRFVVxGX4F21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step10:  Model and Training Setup\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.00001,\n",
        "    logging_dir='./logs',\n",
        "    eval_steps=100\n",
        ")"
      ],
      "metadata": {
        "id": "Orn6ZlH54Y25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with training_args.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=207)\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "w5jZX44n4g-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step11: Train and Evaluate the Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MzQ6DQdO4hdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "a9VM9ysg4jJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step12: Save the Model and Tokenizer\n",
        "save_directory = \"/content/saved_models\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "bzwYWiS84lZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step13: Load the fine-tuned model and tokenizer for inference\n",
        "model_fine_tuned = TFDistilBertForSequenceClassification.from_pretrained(save_directory)\n",
        "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(save_directory)\n",
        "test_text = test_texts[0]"
      ],
      "metadata": {
        "id": "ylfLGcMD4n64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step14: Make a prediction\n",
        "predict_input = tokenizer_fine_tuned.encode(\n",
        "    test_text,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='tf'\n",
        ")"
      ],
      "metadata": {
        "id": "mtxIHTLP4vMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_fine_tuned(predict_input)[0]\n",
        "prediction_value = tf.argmax(output, axis=1).numpy()[0]\n"
      ],
      "metadata": {
        "id": "VE1mTWuD4vIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted class index:\", prediction_value)"
      ],
      "metadata": {
        "id": "bsRZsEI04vGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}